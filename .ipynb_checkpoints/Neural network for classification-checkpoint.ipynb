{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46a2f2f-b91d-46f8-8388-9955c12e9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a7febf-71bb-4b0a-87b4-08777ac3145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0            842     0          2.2         0   1       0           7    0.6   \n",
       "1           1021     1          0.5         1   0       1          53    0.7   \n",
       "2            563     1          0.5         1   2       1          41    0.9   \n",
       "3            615     1          2.5         0   0       0          10    0.8   \n",
       "4           1821     1          1.2         0  13       1          44    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        188        2  ...         20       756  2549     9     7         19   \n",
       "1        136        3  ...        905      1988  2631    17     3          7   \n",
       "2        145        5  ...       1263      1716  2603    11     2          9   \n",
       "3        131        6  ...       1216      1786  2769    16     8         11   \n",
       "4        141        2  ...       1208      1212  1411     8     2         15   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        0             0     1            1  \n",
       "1        1             1     0            2  \n",
       "2        1             1     0            2  \n",
       "3        1             0     0            2  \n",
       "4        1             1     0            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_train = pd.read_csv('mobile_train.csv')\n",
    "mobile_test = pd.read_csv('mobile_test.csv')\n",
    "mobile_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bde4cb3-09e4-4435-a3ca-e26b041c0f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1238.518500</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>1.522250</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>4.309500</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>32.046500</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>140.249000</td>\n",
       "      <td>4.520500</td>\n",
       "      <td>...</td>\n",
       "      <td>645.108000</td>\n",
       "      <td>1251.515500</td>\n",
       "      <td>2124.213000</td>\n",
       "      <td>12.306500</td>\n",
       "      <td>5.767000</td>\n",
       "      <td>11.011000</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>439.418206</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.816004</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>4.341444</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>18.145715</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>35.399655</td>\n",
       "      <td>2.287837</td>\n",
       "      <td>...</td>\n",
       "      <td>443.780811</td>\n",
       "      <td>432.199447</td>\n",
       "      <td>1084.732044</td>\n",
       "      <td>4.213245</td>\n",
       "      <td>4.356398</td>\n",
       "      <td>5.463955</td>\n",
       "      <td>0.426273</td>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.500076</td>\n",
       "      <td>1.118314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>501.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>851.750000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282.750000</td>\n",
       "      <td>874.750000</td>\n",
       "      <td>1207.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1226.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>2146.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1615.250000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>947.250000</td>\n",
       "      <td>1633.000000</td>\n",
       "      <td>3064.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1998.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>3998.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       battery_power       blue  clock_speed     dual_sim           fc  \\\n",
       "count    2000.000000  2000.0000  2000.000000  2000.000000  2000.000000   \n",
       "mean     1238.518500     0.4950     1.522250     0.509500     4.309500   \n",
       "std       439.418206     0.5001     0.816004     0.500035     4.341444   \n",
       "min       501.000000     0.0000     0.500000     0.000000     0.000000   \n",
       "25%       851.750000     0.0000     0.700000     0.000000     1.000000   \n",
       "50%      1226.000000     0.0000     1.500000     1.000000     3.000000   \n",
       "75%      1615.250000     1.0000     2.200000     1.000000     7.000000   \n",
       "max      1998.000000     1.0000     3.000000     1.000000    19.000000   \n",
       "\n",
       "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.521500    32.046500     0.501750   140.249000     4.520500  ...   \n",
       "std       0.499662    18.145715     0.288416    35.399655     2.287837  ...   \n",
       "min       0.000000     2.000000     0.100000    80.000000     1.000000  ...   \n",
       "25%       0.000000    16.000000     0.200000   109.000000     3.000000  ...   \n",
       "50%       1.000000    32.000000     0.500000   141.000000     4.000000  ...   \n",
       "75%       1.000000    48.000000     0.800000   170.000000     7.000000  ...   \n",
       "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
       "\n",
       "         px_height     px_width          ram         sc_h         sc_w  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    645.108000  1251.515500  2124.213000    12.306500     5.767000   \n",
       "std     443.780811   432.199447  1084.732044     4.213245     4.356398   \n",
       "min       0.000000   500.000000   256.000000     5.000000     0.000000   \n",
       "25%     282.750000   874.750000  1207.500000     9.000000     2.000000   \n",
       "50%     564.000000  1247.000000  2146.500000    12.000000     5.000000   \n",
       "75%     947.250000  1633.000000  3064.500000    16.000000     9.000000   \n",
       "max    1960.000000  1998.000000  3998.000000    19.000000    18.000000   \n",
       "\n",
       "         talk_time      three_g  touch_screen         wifi  price_range  \n",
       "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
       "mean     11.011000     0.761500      0.503000     0.507000     1.500000  \n",
       "std       5.463955     0.426273      0.500116     0.500076     1.118314  \n",
       "min       2.000000     0.000000      0.000000     0.000000     0.000000  \n",
       "25%       6.000000     1.000000      0.000000     0.000000     0.750000  \n",
       "50%      11.000000     1.000000      1.000000     1.000000     1.500000  \n",
       "75%      16.000000     1.000000      1.000000     1.000000     2.250000  \n",
       "max      20.000000     1.000000      1.000000     1.000000     3.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e034b0e-c1c2-46af-9bf1-5ca9a7ea2c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   battery_power  2000 non-null   int64  \n",
      " 1   blue           2000 non-null   int64  \n",
      " 2   clock_speed    2000 non-null   float64\n",
      " 3   dual_sim       2000 non-null   int64  \n",
      " 4   fc             2000 non-null   int64  \n",
      " 5   four_g         2000 non-null   int64  \n",
      " 6   int_memory     2000 non-null   int64  \n",
      " 7   m_dep          2000 non-null   float64\n",
      " 8   mobile_wt      2000 non-null   int64  \n",
      " 9   n_cores        2000 non-null   int64  \n",
      " 10  pc             2000 non-null   int64  \n",
      " 11  px_height      2000 non-null   int64  \n",
      " 12  px_width       2000 non-null   int64  \n",
      " 13  ram            2000 non-null   int64  \n",
      " 14  sc_h           2000 non-null   int64  \n",
      " 15  sc_w           2000 non-null   int64  \n",
      " 16  talk_time      2000 non-null   int64  \n",
      " 17  three_g        2000 non-null   int64  \n",
      " 18  touch_screen   2000 non-null   int64  \n",
      " 19  wifi           2000 non-null   int64  \n",
      " 20  price_range    2000 non-null   int64  \n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 328.2 KB\n"
     ]
    }
   ],
   "source": [
    "mobile_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa309d19-39af-4619-aebb-7c8d4ab358d5",
   "metadata": {},
   "source": [
    "Drawing parallels from the previous notebook we will be removing the correlated input variables using VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ae1d5-37cc-46c0-acb8-d045d0ce3907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping feature 'mobile_wt' with VIF 12.972548425819065\n",
      "Dropping feature 'px_width' with VIF 11.470014131904488\n",
      "Dropping feature 'sc_h' with VIF 11.086593845458365\n",
      "Dropping feature 'battery_power' with VIF 7.543843177190293\n",
      "Dropping feature 'pc' with VIF 6.050059878559392\n",
      "Dropping feature 'three_g' with VIF 5.930418164840767\n"
     ]
    }
   ],
   "source": [
    "mobile_train_vif = mobile_train.drop(['price_range'], axis=1)\n",
    "\n",
    "def calculate_vif(data_frame):\n",
    "    features = data_frame.columns\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = features\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data_frame.values, i) for i in range(data_frame.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "    \n",
    "def drop_high_vif_features(data_frame, threshold=5):\n",
    "    while True:\n",
    "        vif_results = calculate_vif(data_frame)\n",
    "        max_vif_feature = vif_results.loc[vif_results['VIF'].idxmax(), 'Feature']\n",
    "        max_vif_value = vif_results.loc[vif_results['VIF'].idxmax(), 'VIF']\n",
    "        \n",
    "        if max_vif_value > threshold:\n",
    "            print(f\"Dropping feature '{max_vif_feature}' with VIF {max_vif_value}\")\n",
    "            data_frame = data_frame.drop(columns=max_vif_feature)\n",
    "        else:\n",
    "            break\n",
    "    return data_frame\n",
    "mobile_train_vif = drop_high_vif_features(mobile_train_vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3dd19c-7505-4448-ae58-f07ced8ccc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vif = mobile_train_vif\n",
    "y_vif = mobile_train['price_range']\n",
    "X_train_vif, X_test_vif, y_train_vif, y_test_vif = train_test_split(X_vif, y_vif, test_size=0.2, random_state=42)\n",
    "## now lets standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_vif_scaled = scaler.fit_transform(X_train_vif)\n",
    "X_test_vif_scaled = scaler.transform(X_test_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6f673-ee3e-4ae2-a549-977717530abd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01c4cd-422c-4f27-b110-2cbd30901e6f",
   "metadata": {},
   "source": [
    "We will predict results using a base case logistic regression model from sk learn library and use it for benchmarking as we move forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408fd20a-26f9-4f10-a93a-a528d6bc2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Confusion Matrix:\n",
      "[[93 12  0  0]\n",
      " [12 61 18  0]\n",
      " [ 0 10 68 14]\n",
      " [ 0  0 18 94]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       105\n",
      "           1       0.73      0.67      0.70        91\n",
      "           2       0.65      0.74      0.69        92\n",
      "           3       0.87      0.84      0.85       112\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.79      0.78      0.78       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_vif = LogisticRegression()\n",
    "model_vif.fit(X_train_vif_scaled, y_train_vif)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_vif = model_vif.predict(X_test_vif_scaled)\n",
    "\n",
    "# Check accuracy of model\n",
    "accuracy_lr = accuracy_score(y_test_vif, y_pred_vif)\n",
    "conf_matrix_lr = confusion_matrix(y_test_vif, y_pred_vif)\n",
    "classification_rep_lr = classification_report(y_test_vif, y_pred_vif)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lr}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix_lr}\")\n",
    "print(f\"Classification Report:\\n{classification_rep_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bd1ef-f78c-4de4-bd67-c959ed981ab3",
   "metadata": {},
   "source": [
    "### Logistic Regression - Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286a2764-3888-42c4-a8f3-5a0e32b3747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the data into dimensions (n,m) where m denoted the number of examples for ease of computation\n",
    "X_train_nn, X_test_nn = X_train_vif_scaled.T, X_test_vif_scaled.T\n",
    "y_train_nn,y_test_nn = y_train_vif.to_numpy().reshape((1,1600)), y_test_vif.to_numpy().reshape((1,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b92782-0d14-4c8e-8efc-833fcc8a8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def initialize_parameters(n, num_classes):\n",
    "    w = np.random.randn(n, num_classes) * 0.01\n",
    "    b = np.zeros((num_classes, 1))\n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = softmax(Z)\n",
    "    cost = -1/m * np.sum(np.log(A[Y, np.arange(m)]))\n",
    "    \n",
    "    # Backward propagation\n",
    "    dz = A.copy()\n",
    "    dz[Y, np.arange(m)] -= 1\n",
    "    dw = 1/m * np.dot(X, dz.T)\n",
    "    db = 1/m * np.sum(dz, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= learning_rate * grads[\"dw\"]\n",
    "        b -= learning_rate * grads[\"db\"]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def predict(w, b, X):\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = softmax(Z)\n",
    "    predictions = np.argmax(A, axis=0)\n",
    "    return predictions\n",
    "\n",
    "# we will try to encompass all of the above into one function called lr_nn_model\n",
    "\n",
    "def lr_nn_model(X_train, Y_train, num_classes, num_iterations=1000, learning_rate=0.01):\n",
    "    w, b = initialize_parameters(X_train.shape[0], num_classes)\n",
    "    w, b = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    return w, b\n",
    "    \n",
    "def accuracy(predictions, actual_labels):\n",
    "    correct_predictions = np.sum(predictions == actual_labels)\n",
    "    total_examples = len(actual_labels[0])\n",
    "    acc = correct_predictions / total_examples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25b9790-f628-486c-b20e-cd40f518808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 0.69875\n",
      "Confusion Matrix Train:\n",
      "[[384  11   0   0]\n",
      " [161 184  56   8]\n",
      " [  6  62 175 165]\n",
      " [  0   0  13 375]]\n",
      "\n",
      "Classification Report Train:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.97      0.81       395\n",
      "           1       0.72      0.45      0.55       409\n",
      "           2       0.72      0.43      0.54       408\n",
      "           3       0.68      0.97      0.80       388\n",
      "\n",
      "    accuracy                           0.70      1600\n",
      "   macro avg       0.70      0.70      0.68      1600\n",
      "weighted avg       0.70      0.70      0.67      1600\n",
      "\n",
      "Accuracy: 0.72\n",
      "Confusion Matrix:\n",
      "[[105   0   0   0]\n",
      " [ 33  40  17   1]\n",
      " [  0  11  38  43]\n",
      " [  0   0   7 105]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       105\n",
      "           1       0.78      0.44      0.56        91\n",
      "           2       0.61      0.41      0.49        92\n",
      "           3       0.70      0.94      0.80       112\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.72      0.70      0.68       400\n",
      "weighted avg       0.72      0.72      0.69       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w, b = lr_nn_model(X_train_nn,y_train_nn, 4)\n",
    "train_predictions = predict(w,b,X_train_nn)\n",
    "accuracy_scores = accuracy(train_predictions, y_train_nn)\n",
    "\n",
    "test_predictions = predict(w, b, X_test_nn)\n",
    "accuracy_scores_test = accuracy(test_predictions, y_test_nn)\n",
    "\n",
    "cm1 = confusion_matrix(np.squeeze(y_train_nn), np.squeeze(train_predictions))\n",
    "class_metrics1 = classification_report(np.squeeze(y_train_nn), np.squeeze(train_predictions))\n",
    "cm = confusion_matrix(np.squeeze(y_test_nn), np.squeeze(test_predictions))\n",
    "class_metrics = classification_report(np.squeeze(y_test_nn), np.squeeze(test_predictions))\n",
    "\n",
    "print(f\"Accuracy Train: {accuracy_scores}\")\n",
    "print(\"Confusion Matrix Train:\")\n",
    "print(cm1)\n",
    "\n",
    "print(\"\\nClassification Report Train:\")\n",
    "print(class_metrics1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_scores_test}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff589c2-8c96-4bb5-873d-849daa0eac66",
   "metadata": {},
   "source": [
    "### Neural Network with 1 Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb8210-895c-48b1-824c-0470112ff29d",
   "metadata": {},
   "source": [
    "Now lets build further on the neural network implementation of logistic regression and add one hidden layer to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16fddc9-a6f8-42dc-8b0c-4b85a5155b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def initialize_parameters(n, n_hidden, num_classes):\n",
    "    w1 = np.random.randn(n_hidden, n) * np.sqrt(2 / n)  # Xavier initialization\n",
    "    b1 = np.zeros((n_hidden, 1))\n",
    "    w2 = np.random.randn(num_classes, n_hidden) * np.sqrt(2 / n_hidden)  # Xavier initialization\n",
    "    b2 = np.zeros((num_classes, 1))\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def forward_propagation(w1, b1, w2, b2, X):\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "def backward_propagation(A1, A2, w2, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # dz2 = A2 - Y\n",
    "    dz2 = A2.copy()\n",
    "    dz2[Y, np.arange(m)] -= 1\n",
    "    dw2 = 1/m * np.dot(dz2, A1.T)\n",
    "    db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)\n",
    "    \n",
    "    dz1 = np.dot(w2.T, dz2) * relu_derivative(A1)\n",
    "    dw1 = 1/m * np.dot(dz1, X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dw1\": dw1, \"db1\": db1, \"dw2\": dw2, \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(w1, b1, w2, b2, grads, learning_rate):\n",
    "    w1 -= learning_rate * grads[\"dw1\"]\n",
    "    b1 -= learning_rate * grads[\"db1\"]\n",
    "    w2 -= learning_rate * grads[\"dw2\"]\n",
    "    b2 -= learning_rate * grads[\"db2\"]\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def predict(w1, b1, w2, b2, X):\n",
    "    _, A2 = forward_propagation(w1, b1, w2, b2, X)\n",
    "    predictions = np.argmax(A2, axis=0)\n",
    "    return predictions\n",
    "\n",
    "def nn_1_layer_model(X_train, Y_train, X_test, Y_test, num_classes, n_hidden, num_iterations=1000, learning_rate=0.01):\n",
    "    # Initialize parameters\n",
    "    w1, b1, w2, b2 = initialize_parameters(X_train.shape[0], n_hidden, num_classes)\n",
    "\n",
    "    # Train the neural network\n",
    "    for i in range(num_iterations):\n",
    "        A1, A2 = forward_propagation(w1, b1, w2, b2, X_train)\n",
    "        grads = backward_propagation(A1, A2, w2, X_train, Y_train)\n",
    "        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, grads, learning_rate)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = predict(w1, b1, w2, b2, X_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = predict(w1, b1, w2, b2, X_test)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_test), np.squeeze(test_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_test), np.squeeze(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060d28eb-786a-4e60-8491-0f666f59b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[369  17   9   0]\n",
      " [161 133 107   8]\n",
      " [  9  60 162 177]\n",
      " [  0   0  11 377]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.93      0.79       395\n",
      "           1       0.63      0.33      0.43       409\n",
      "           2       0.56      0.40      0.46       408\n",
      "           3       0.67      0.97      0.79       388\n",
      "\n",
      "    accuracy                           0.65      1600\n",
      "   macro avg       0.64      0.66      0.62      1600\n",
      "weighted avg       0.64      0.65      0.62      1600\n",
      "\n",
      "\n",
      "Test Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[ 97   5   3   0]\n",
      " [ 44  26  18   3]\n",
      " [  1  12  37  42]\n",
      " [  0   0   9 103]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.92      0.79       105\n",
      "           1       0.60      0.29      0.39        91\n",
      "           2       0.55      0.40      0.47        92\n",
      "           3       0.70      0.92      0.79       112\n",
      "\n",
      "    accuracy                           0.66       400\n",
      "   macro avg       0.63      0.63      0.61       400\n",
      "weighted avg       0.64      0.66      0.62       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_1_layer_model(X_train_nn, y_train_nn, X_test_nn, y_test_nn, num_classes=4, n_hidden=4, num_iterations=10000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60334e6b-e016-49ea-850f-b8998f44208c",
   "metadata": {},
   "source": [
    "Although we get reasonalbly good results in test set, our model isnt able to fit the training set well enough. so lets focus on that and get it better by running a deeper neural networ model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8ed83-f2dd-47d0-8d1c-6738da92dddb",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d49d286-c329-4ed8-b198-ae21d05fa76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def activation_forward(Z, activation):\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation_deep(X, parameters, activations):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z, linear_cache = linear_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'])\n",
    "        A, activation_cache = activation_forward(Z, \"relu\")\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Last layer (softmax activation)\n",
    "    ZL, linear_cache = linear_forward(A, parameters[f'W{L}'], parameters[f'b{L}'])\n",
    "    AL, activation_cache = activation_forward(ZL, \"softmax\")\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    epsilon = 1e-8\n",
    "    cost = -1/m * np.sum(Y * np.log(AL+epsilon))\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    Z = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = dA * relu_derivative(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def backward_propagation_deep(AL, Y, caches, activations):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "\n",
    "    # Convert Y to one-hot encoded matrix\n",
    "    Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "\n",
    "    # Compute gradient of the cost with respect to AL for softmax activation\n",
    "    dAL = AL - Y_one_hot\n",
    "    grads['dAL'] = dAL\n",
    "    \n",
    "    # Last layer (softmax activation)\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, activation_cache = current_cache\n",
    "    dZL = activation_backward(dAL, activation_cache, \"softmax\")\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = linear_backward(dZL, linear_cache)\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        linear_cache, activation_cache = current_cache\n",
    "        dZ = activation_backward(grads[f'dA{l+1}'], activation_cache, \"relu\")\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters_deep(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameters[f'W{l}'] -= learning_rate * grads[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * grads[f'db{l}']\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predict_deep(parameters, X, activations):\n",
    "    AL, _ = forward_propagation_deep(X, parameters, activations)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions\n",
    "\n",
    "def nn_deep_model(X_train, Y_train, X_test, Y_test, layer_dims, num_iterations=1000, learning_rate=0.01):\n",
    "    activations = [\"relu\"] * (len(layer_dims) - 2) + [\"softmax\"]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "    # Train the neural network\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation_deep(X_train, parameters, activations)\n",
    "        cost = compute_cost(AL, Y_train)\n",
    "        grads = backward_propagation_deep(AL, Y_train, caches, activations)\n",
    "        parameters = update_parameters_deep(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            print(f'Mean activation: {np.mean(AL)}, Std activation: {np.std(AL)}')\n",
    "            print(f'Mean dAL: {np.mean(grads[\"dAL\"])}, Std dAL: {np.std(grads[\"dAL\"])}')\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = predict_deep(parameters, X_train, activations)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = predict_deep(parameters, X_test, activations)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_test), np.squeeze(test_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_test), np.squeeze(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3e33ab-259e-43fa-b303-410167345f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 20.008478358476225\n",
      "Mean activation: 0.25, Std activation: 0.2961670521900114\n",
      "Mean dAL: -8.881784197001253e-18, Std dAL: 0.524028274987026\n",
      "Cost after iteration 1000: 8.341292123526994\n",
      "Mean activation: 0.25, Std activation: 0.03878712720285193\n",
      "Mean dAL: 2.636779683484747e-18, Std dAL: 0.4309586879542949\n",
      "Cost after iteration 2000: 8.474325574292818\n",
      "Mean activation: 0.25, Std activation: 0.09410157889364992\n",
      "Mean dAL: 4.440892098500626e-18, Std dAL: 0.41219443415496354\n",
      "Cost after iteration 3000: 9.58065498992225\n",
      "Mean activation: 0.25, Std activation: 0.19990847004585763\n",
      "Mean dAL: -2.220446049250313e-18, Std dAL: 0.3576117044364538\n",
      "Cost after iteration 4000: 11.019415620341112\n",
      "Mean activation: 0.25, Std activation: 0.2289854449824148\n",
      "Mean dAL: 0.0, Std dAL: 0.3355943842223098\n",
      "Cost after iteration 5000: 12.243119703145654\n",
      "Mean activation: 0.25, Std activation: 0.2541385634425432\n",
      "Mean dAL: 0.0, Std dAL: 0.311932044478622\n",
      "Training Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[352  43   0   0]\n",
      " [ 55 297  57   0]\n",
      " [  0  68 274  66]\n",
      " [  0   0  37 351]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88       395\n",
      "           1       0.73      0.73      0.73       409\n",
      "           2       0.74      0.67      0.71       408\n",
      "           3       0.84      0.90      0.87       388\n",
      "\n",
      "    accuracy                           0.80      1600\n",
      "   macro avg       0.79      0.80      0.80      1600\n",
      "weighted avg       0.79      0.80      0.79      1600\n",
      "\n",
      "\n",
      "Test Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[94 11  0  0]\n",
      " [14 64 13  0]\n",
      " [ 0 11 66 15]\n",
      " [ 0  0 16 96]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       105\n",
      "           1       0.74      0.70      0.72        91\n",
      "           2       0.69      0.72      0.71        92\n",
      "           3       0.86      0.86      0.86       112\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.79      0.79      0.79       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "layer_dims = [14, 4, 3, 4, 4]\n",
    "nn_deep_model(X_train_nn, y_train_nn, X_test_nn, y_test_nn, layer_dims, num_iterations=6000, learning_rate=0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ac662-9196-4f31-bed9-9f680440b55c",
   "metadata": {},
   "source": [
    "The Deep neural network model seems to performing considerably better than the initial 2 layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e7b9d-2ad9-4687-a62b-ebb1ba544dae",
   "metadata": {},
   "source": [
    "This is evident from the improvement in training performance, but even then it has a pretty high bias. If we consider max achievable performance is 100% (which most probably might not be the case in most scenarios and the human level performance will be lesser than 100% but due to lack of conclusive data about human level performance we will assume that it is 100%), then the deep NN model still has quite some scope for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459f9c2-6d9a-4770-9271-75f0e12478a3",
   "metadata": {},
   "source": [
    "Let's add Batch normalization to the activations of each layer and then feed it forward to the next layer, this helps in making the gradient descent more faster and computationally simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f0b6c-8d17-4f44-a8f8-2b91626d5fbd",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a8a2ec-b6c7-4c3e-944d-280462ad1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def batch_normalize(Z):\n",
    "    mean = np.mean(Z, axis=1, keepdims=True)\n",
    "    var = np.var(Z, axis=1, keepdims=True)\n",
    "    Z_normalized = (Z - mean) / np.sqrt(var + 1e-8)\n",
    "    return Z_normalized, mean, var\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        parameters[f'gamma{l}'] = np.ones((layer_dims[l], 1))\n",
    "        parameters[f'beta{l}'] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def batch_normalize_forward(Z, gamma, beta):\n",
    "    Z_normalized, mean, var = batch_normalize(Z)\n",
    "    Z_tilde = gamma * Z_normalized + beta\n",
    "    cache = (Z, Z_normalized, mean, var, gamma, beta)\n",
    "    return Z_tilde, cache\n",
    "\n",
    "def activation_forward(Z, activation):\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation_deep(X, parameters, activations):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z, linear_cache = linear_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'])\n",
    "        Z_tilde, batch_cache = batch_normalize_forward(Z, parameters[f'gamma{l}'], parameters[f'beta{l}'])\n",
    "        A, activation_cache = activation_forward(Z_tilde, \"relu\")\n",
    "        cache = (linear_cache, batch_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Last layer (softmax activation)\n",
    "    ZL, linear_cache = linear_forward(A, parameters[f'W{L}'], parameters[f'b{L}'])\n",
    "    ZL_tilde, batch_cache = batch_normalize_forward(ZL, parameters[f'gamma{L}'], parameters[f'beta{L}'])\n",
    "    AL, activation_cache = activation_forward(ZL_tilde, \"softmax\")\n",
    "    cache = (linear_cache, batch_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    epsilon = 1e-8\n",
    "    cost = -1/m * np.sum(Y * np.log(AL+epsilon))\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def batch_normalize_backward(dZ_tilde, cache):\n",
    "    Z, Z_normalized, mean, var, gamma, beta = cache\n",
    "    m = Z.shape[1]\n",
    "\n",
    "    dZ_normalized = dZ_tilde * gamma\n",
    "    dVar = np.sum(dZ_normalized * (Z - mean), axis=1, keepdims=True) * -0.5 * (var + 1e-8)**(-1.5)\n",
    "    dMean = np.sum(dZ_normalized, axis=1, keepdims=True) * -1 / np.sqrt(var + 1e-8)\n",
    "    \n",
    "    dZ = (dZ_normalized / np.sqrt(var + 1e-8)) + (dVar * 2 * (Z - mean) / m) + (dMean / m)\n",
    "    dGamma = np.sum(dZ_tilde * Z_normalized, axis=1, keepdims=True)\n",
    "    dBeta = np.sum(dZ_tilde, axis=1, keepdims=True)\n",
    "\n",
    "    return dZ, dGamma, dBeta\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    Z = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = dA * relu_derivative(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def backward_propagation_deep(AL, Y, caches, activations):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "\n",
    "    # Convert Y to one-hot encoded matrix\n",
    "    Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "\n",
    "    # Compute gradient of the cost with respect to AL for softmax activation\n",
    "    dAL = AL - Y_one_hot\n",
    "    grads['dAL'] = dAL\n",
    "    \n",
    "    # Last layer (softmax activation)\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, batch_cache, activation_cache = current_cache\n",
    "    dZL_tilde = activation_backward(dAL, activation_cache, \"softmax\")\n",
    "    dZL, dGammaL, dBetaL = batch_normalize_backward(dZL_tilde, batch_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZL, linear_cache)\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = dA_prev, dW, db\n",
    "    grads[f'dGamma{L}'], grads[f'dBeta{L}'] = dGammaL, dBetaL\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        linear_cache, batch_cache, activation_cache = current_cache\n",
    "        dZ_tilde = activation_backward(grads[f'dA{l+1}'], activation_cache, \"relu\")\n",
    "        dZ, dGamma, dBeta = batch_normalize_backward(dZ_tilde, batch_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = dA_prev, dW, db\n",
    "        grads[f'dGamma{l+1}'], grads[f'dBeta{l+1}'] = dGamma, dBeta\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters_deep(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 4\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameters[f'W{l}'] -= learning_rate * grads[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * grads[f'db{l}']\n",
    "        parameters[f'gamma{l}'] -= learning_rate * grads[f'dGamma{l}']\n",
    "        parameters[f'beta{l}'] -= learning_rate * grads[f'dBeta{l}']\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predict_deep(parameters, X, activations):\n",
    "    AL, _ = forward_propagation_deep(X, parameters, activations)\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions\n",
    "\n",
    "def nn_deep_model(X_train, Y_train, X_test, Y_test, layer_dims, num_iterations=1000, learning_rate=0.01):\n",
    "    activations = [\"relu\"] * (len(layer_dims) - 2) + [\"softmax\"]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "    # Train the neural network\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation_deep(X_train, parameters, activations)\n",
    "        cost = compute_cost(AL, Y_train)\n",
    "        grads = backward_propagation_deep(AL, Y_train, caches, activations)\n",
    "        parameters = update_parameters_deep(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            print(f'Mean activation: {np.mean(AL)}, Std activation: {np.std(AL)}')\n",
    "            print(f'Mean dAL: {np.mean(grads[\"dAL\"])}, Std dAL: {np.std(grads[\"dAL\"])}')\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = predict_deep(parameters, X_train, activations)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = predict_deep(parameters, X_test, activations)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_test), np.squeeze(test_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_test), np.squeeze(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dfee940-e3d4-4042-9613-1c3f329ec436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 9.704612955134582\n",
      "Mean activation: 0.25, Std activation: 0.14852017421167846\n",
      "Mean dAL: 2.220446049250313e-18, Std dAL: 0.44923822339161956\n",
      "Cost after iteration 1000: 43.63739568544343\n",
      "Mean activation: 0.25, Std activation: 0.37347116953845233\n",
      "Mean dAL: 0.0, Std dAL: 0.30170411648425216\n",
      "Cost after iteration 2000: 43.1767925672801\n",
      "Mean activation: 0.25, Std activation: 0.3727114793110046\n",
      "Mean dAL: 0.0, Std dAL: 0.29843203939959495\n",
      "Cost after iteration 3000: 42.27450680621344\n",
      "Mean activation: 0.25, Std activation: 0.3718705910066784\n",
      "Mean dAL: 4.440892098500626e-18, Std dAL: 0.29503333376730845\n",
      "Cost after iteration 4000: 41.31102174275683\n",
      "Mean activation: 0.24999999999999997, Std activation: 0.37075578200473586\n",
      "Mean dAL: 2.220446049250313e-18, Std dAL: 0.29210323122166887\n",
      "Cost after iteration 5000: 40.43894533642438\n",
      "Mean activation: 0.25, Std activation: 0.3698821540673365\n",
      "Mean dAL: -2.220446049250313e-18, Std dAL: 0.2888763479064268\n",
      "Training Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[324  71   0   0]\n",
      " [ 27 362  20   0]\n",
      " [  0 124 182 102]\n",
      " [  0   0  25 363]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87       395\n",
      "           1       0.65      0.89      0.75       409\n",
      "           2       0.80      0.45      0.57       408\n",
      "           3       0.78      0.94      0.85       388\n",
      "\n",
      "    accuracy                           0.77      1600\n",
      "   macro avg       0.79      0.77      0.76      1600\n",
      "weighted avg       0.79      0.77      0.76      1600\n",
      "\n",
      "\n",
      "Test Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[83 22  0  0]\n",
      " [ 6 82  3  0]\n",
      " [ 0 31 43 18]\n",
      " [ 0  0 13 99]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.86       105\n",
      "           1       0.61      0.90      0.73        91\n",
      "           2       0.73      0.47      0.57        92\n",
      "           3       0.85      0.88      0.86       112\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.78      0.76      0.75       400\n",
      "weighted avg       0.79      0.77      0.76       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "layer_dims = [X_train_nn.shape[0], 10, 8, 6, 4] \n",
    "\n",
    "# Train the neural network with batch normalization\n",
    "nn_deep_model(X_train_nn, y_train_nn, X_test_nn, y_test_nn, layer_dims, num_iterations=6000, learning_rate=0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be004f6-4d00-48a2-8534-42468f150965",
   "metadata": {},
   "source": [
    "Adding batch normalization has helped the model train faster and computationally more efficient compared to before. However, there is still the issue of high bias in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccbe85-911a-4b4a-a2f1-c3177d207035",
   "metadata": {},
   "source": [
    "So we will try to reduce the bias by trying out a different optimization technique - ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef693f-437a-4189-b5ea-a54167fd7d5c",
   "metadata": {},
   "source": [
    "### ADAM Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2559dec-6b5b-4fed-8aaf-c46677b0753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def batch_normalize(Z):\n",
    "    mean = np.mean(Z, axis=1, keepdims=True)\n",
    "    var = np.var(Z, axis=1, keepdims=True)\n",
    "    Z_normalized = (Z - mean) / np.sqrt(var + 1e-8)\n",
    "    return Z_normalized, mean, var\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        parameters[f'gamma{l}'] = np.ones((layer_dims[l], 1))\n",
    "        parameters[f'beta{l}'] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def batch_normalize_forward(Z, gamma, beta):\n",
    "    Z_normalized, mean, var = batch_normalize(Z)\n",
    "    Z_tilde = gamma * Z_normalized + beta\n",
    "    cache = (Z, Z_normalized, mean, var, gamma, beta)\n",
    "    return Z_tilde, cache\n",
    "\n",
    "def activation_forward(Z, activation):\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation_deep(X, parameters, activations):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z, linear_cache = linear_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'])\n",
    "        Z_tilde, batch_cache = batch_normalize_forward(Z, parameters[f'gamma{l}'], parameters[f'beta{l}'])\n",
    "        A, activation_cache = activation_forward(Z_tilde, \"relu\")\n",
    "        cache = (linear_cache, batch_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Last layer (softmax activation)\n",
    "    ZL, linear_cache = linear_forward(A, parameters[f'W{L}'], parameters[f'b{L}'])\n",
    "    ZL_tilde, batch_cache = batch_normalize_forward(ZL, parameters[f'gamma{L}'], parameters[f'beta{L}'])\n",
    "    AL, activation_cache = activation_forward(ZL_tilde, \"softmax\")\n",
    "    cache = (linear_cache, batch_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    epsilon = 1e-8\n",
    "    cost = -1/m * np.sum(Y * np.log(AL+epsilon))\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def batch_normalize_backward(dZ_tilde, cache):\n",
    "    Z, Z_normalized, mean, var, gamma, beta = cache\n",
    "    m = Z.shape[1]\n",
    "\n",
    "    dZ_normalized = dZ_tilde * gamma\n",
    "    dVar = np.sum(dZ_normalized * (Z - mean), axis=1, keepdims=True) * -0.5 * (var + 1e-8)**(-1.5)\n",
    "    dMean = np.sum(dZ_normalized, axis=1, keepdims=True) * -1 / np.sqrt(var + 1e-8)\n",
    "    \n",
    "    dZ = (dZ_normalized / np.sqrt(var + 1e-8)) + (dVar * 2 * (Z - mean) / m) + (dMean / m)\n",
    "    dGamma = np.sum(dZ_tilde * Z_normalized, axis=1, keepdims=True)\n",
    "    dBeta = np.sum(dZ_tilde, axis=1, keepdims=True)\n",
    "\n",
    "    return dZ, dGamma, dBeta\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    Z = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = dA * relu_derivative(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def backward_propagation_deep(AL, Y, caches, activations):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "\n",
    "    # Convert Y to one-hot encoded matrix\n",
    "    Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "\n",
    "    # Compute gradient of the cost with respect to AL for softmax activation\n",
    "    dAL = AL - Y_one_hot\n",
    "    grads['dAL'] = dAL\n",
    "    \n",
    "    # Last layer (softmax activation)\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, batch_cache, activation_cache = current_cache\n",
    "    dZL_tilde = activation_backward(dAL, activation_cache, \"softmax\")\n",
    "    dZL, dGammaL, dBetaL = batch_normalize_backward(dZL_tilde, batch_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZL, linear_cache)\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = dA_prev, dW, db\n",
    "    grads[f'dGamma{L}'], grads[f'dBeta{L}'] = dGammaL, dBetaL\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        linear_cache, batch_cache, activation_cache = current_cache\n",
    "        dZ_tilde = activation_backward(grads[f'dA{l+1}'], activation_cache, \"relu\")\n",
    "        dZ, dGamma, dBeta = batch_normalize_backward(dZ_tilde, batch_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = dA_prev, dW, db\n",
    "        grads[f'dGamma{l+1}'], grads[f'dBeta{l+1}'] = dGamma, dBeta\n",
    "\n",
    "    return grads\n",
    "\n",
    "def initialize_adam(parameters):\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n",
    "        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n",
    "        v[f'dGamma{l}'] = np.zeros_like(parameters[f'gamma{l}'])\n",
    "        v[f'dBeta{l}'] = np.zeros_like(parameters[f'beta{l}'])\n",
    "\n",
    "        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n",
    "        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n",
    "        s[f'dGamma{l}'] = np.zeros_like(parameters[f'gamma{l}'])\n",
    "        s[f'dBeta{l}'] = np.zeros_like(parameters[f'beta{l}'])\n",
    "\n",
    "    return v, s\n",
    "\n",
    "def update_parameters_adam(parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        v[f'dW{l}'] = beta1 * v[f'dW{l}'] + (1 - beta1) * grads[f'dW{l}']\n",
    "        v[f'db{l}'] = beta1 * v[f'db{l}'] + (1 - beta1) * grads[f'db{l}']\n",
    "        v[f'dGamma{l}'] = beta1 * v[f'dGamma{l}'] + (1 - beta1) * grads[f'dGamma{l}']\n",
    "        v[f'dBeta{l}'] = beta1 * v[f'dBeta{l}'] + (1 - beta1) * grads[f'dBeta{l}']\n",
    "\n",
    "        s[f'dW{l}'] = beta2 * s[f'dW{l}'] + (1 - beta2) * (grads[f'dW{l}']**2)\n",
    "        s[f'db{l}'] = beta2 * s[f'db{l}'] + (1 - beta2) * (grads[f'db{l}']**2)\n",
    "        s[f'dGamma{l}'] = beta2 * s[f'dGamma{l}'] + (1 - beta2) * (grads[f'dGamma{l}']**2)\n",
    "        s[f'dBeta{l}'] = beta2 * s[f'dBeta{l}'] + (1 - beta2) * (grads[f'dBeta{l}']**2)\n",
    "\n",
    "        v_corrected_dW = v[f'dW{l}'] / (1 - beta1**t)\n",
    "        v_corrected_db = v[f'db{l}'] / (1 - beta1**t)\n",
    "        v_corrected_dGamma = v[f'dGamma{l}'] / (1 - beta1**t)\n",
    "        v_corrected_dBeta = v[f'dBeta{l}'] / (1 - beta1**t)\n",
    "\n",
    "        s_corrected_dW = s[f'dW{l}'] / (1 - beta2**t)\n",
    "        s_corrected_db = s[f'db{l}'] / (1 - beta2**t)\n",
    "        s_corrected_dGamma = s[f'dGamma{l}'] / (1 - beta2**t)\n",
    "        s_corrected_dBeta = s[f'dBeta{l}'] / (1 - beta2**t)\n",
    "\n",
    "        parameters[f'W{l}'] -= learning_rate * v_corrected_dW / (np.sqrt(s_corrected_dW) + epsilon)\n",
    "        parameters[f'b{l}'] -= learning_rate * v_corrected_db / (np.sqrt(s_corrected_db) + epsilon)\n",
    "        parameters[f'gamma{l}'] -= learning_rate * v_corrected_dGamma / (np.sqrt(s_corrected_dGamma) + epsilon)\n",
    "        parameters[f'beta{l}'] -= learning_rate * v_corrected_dBeta / (np.sqrt(s_corrected_dBeta) + epsilon)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def nn_deep_model_adam(X_train, Y_train, X_test, Y_test, layer_dims, num_iterations=1000, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    activations = [\"relu\"] * (len(layer_dims) - 2) + [\"softmax\"]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "    # Initialize Adam variables\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0  # Initialize timestep for Adam\n",
    "\n",
    "    # Train the neural network with Adam\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation_deep(X_train, parameters, activations)\n",
    "        cost = compute_cost(AL, Y_train)\n",
    "        grads = backward_propagation_deep(AL, Y_train, caches, activations)\n",
    "\n",
    "        # Update parameters with Adam\n",
    "        t += 1\n",
    "        parameters = update_parameters_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            print(f'Mean activation: {np.mean(AL)}, Std activation: {np.std(AL)}')\n",
    "            print(f'Mean dAL: {np.mean(grads[\"dAL\"])}, Std dAL: {np.std(grads[\"dAL\"])}')\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = predict_deep(parameters, X_train, activations)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = predict_deep(parameters, X_test, activations)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_test), np.squeeze(test_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_test), np.squeeze(test_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728bc7a9-08b9-4995-97bc-8bb02d7825b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 9.704612955134582\n",
      "Mean activation: 0.25, Std activation: 0.14852017421167846\n",
      "Mean dAL: 2.220446049250313e-18, Std dAL: 0.44923822339161956\n",
      "Cost after iteration 1000: 27.163196930973413\n",
      "Mean activation: 0.25, Std activation: 0.38431330923324136\n",
      "Mean dAL: -7.632783294297951e-19, Std dAL: 0.18648655180860438\n",
      "Cost after iteration 2000: 32.43085717950673\n",
      "Mean activation: 0.25, Std activation: 0.3891916307806975\n",
      "Mean dAL: -1.3877787807814457e-19, Std dAL: 0.18358098909584872\n",
      "Cost after iteration 3000: 35.53795136512768\n",
      "Mean activation: 0.25, Std activation: 0.3892691379762523\n",
      "Mean dAL: 0.0, Std dAL: 0.1805235371308559\n",
      "Cost after iteration 4000: 37.850974073243755\n",
      "Mean activation: 0.25, Std activation: 0.39245350057915673\n",
      "Mean dAL: -6.938893903907229e-20, Std dAL: 0.17596205253203348\n",
      "Cost after iteration 5000: 40.91729232353188\n",
      "Mean activation: 0.25, Std activation: 0.39382815161888757\n",
      "Mean dAL: 5.551115123125783e-19, Std dAL: 0.17520993340011268\n",
      "Training Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[393   1   1   0]\n",
      " [ 10 382  17   0]\n",
      " [  2  17 351  38]\n",
      " [  0   0  47 341]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       395\n",
      "           1       0.95      0.93      0.94       409\n",
      "           2       0.84      0.86      0.85       408\n",
      "           3       0.90      0.88      0.89       388\n",
      "\n",
      "    accuracy                           0.92      1600\n",
      "   macro avg       0.92      0.92      0.92      1600\n",
      "weighted avg       0.92      0.92      0.92      1600\n",
      "\n",
      "\n",
      "Test Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[92 13  0  0]\n",
      " [17 63 11  0]\n",
      " [ 0 37 42 13]\n",
      " [ 0  5 21 86]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       105\n",
      "           1       0.53      0.69      0.60        91\n",
      "           2       0.57      0.46      0.51        92\n",
      "           3       0.87      0.77      0.82       112\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.70      0.70      0.70       400\n",
      "weighted avg       0.72      0.71      0.71       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "nn_deep_model_adam(X_train_nn, y_train_nn, X_test_nn, y_test_nn, layer_dims, num_iterations=6000, learning_rate=0.009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec3a90-df10-4a48-8c54-cacd28dfe67d",
   "metadata": {},
   "source": [
    "Now we see an improvement in our training set performance which we weren't able to achieve before and there is a considerable reduction in the bias. Now we can shift our focus to the generalization of the model and the reduction of high variance (which is evident from the huge difference between the training set and test set performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db935528-3bb0-44b9-b4bd-c990e6c403e0",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "828f841b-9027-4a86-aeba-3203029fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def batch_normalize(Z):\n",
    "    mean = np.mean(Z, axis=1, keepdims=True)\n",
    "    var = np.var(Z, axis=1, keepdims=True)\n",
    "    Z_normalized = (Z - mean) / np.sqrt(var + 1e-8)\n",
    "    return Z_normalized, mean, var\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "        parameters[f'gamma{l}'] = np.ones((layer_dims[l], 1))\n",
    "        parameters[f'beta{l}'] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def batch_normalize_forward(Z, gamma, beta):\n",
    "    Z_normalized, mean, var = batch_normalize(Z)\n",
    "    Z_tilde = gamma * Z_normalized + beta\n",
    "    cache = (Z, Z_normalized, mean, var, gamma, beta)\n",
    "    return Z_tilde, cache\n",
    "\n",
    "def activation_forward(Z, activation):\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def forward_propagation_deep(X, parameters, activations):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z, linear_cache = linear_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'])\n",
    "        Z_tilde, batch_cache = batch_normalize_forward(Z, parameters[f'gamma{l}'], parameters[f'beta{l}'])\n",
    "        A, activation_cache = activation_forward(Z_tilde, \"relu\")\n",
    "        cache = (linear_cache, batch_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Last layer (softmax activation)\n",
    "    ZL, linear_cache = linear_forward(A, parameters[f'W{L}'], parameters[f'b{L}'])\n",
    "    ZL_tilde, batch_cache = batch_normalize_forward(ZL, parameters[f'gamma{L}'], parameters[f'beta{L}'])\n",
    "    AL, activation_cache = activation_forward(ZL_tilde, \"softmax\")\n",
    "    cache = (linear_cache, batch_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y, parameters, lambd=0):\n",
    "    m = Y.shape[1]\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    cross_entropy_cost = -1/m * np.sum(Y * np.log(AL+epsilon))\n",
    "    l2_regularization_cost = 0\n",
    "\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "    for l in range(1, L+1):\n",
    "        W = parameters[f'W{l}']\n",
    "        l2_regularization_cost += np.sum(W**2)\n",
    "\n",
    "    l2_regularization_cost *= (lambd / (2 * m))\n",
    "    cost = cross_entropy_cost + l2_regularization_cost\n",
    "\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache, lambd=0):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T) + (lambd / m) * W\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_propagation_deep(AL, Y, caches, activations, lambd=0):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "\n",
    "    # Convert Y to one-hot encoded matrix\n",
    "    Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "\n",
    "    # Compute gradient of the cost with respect to AL for softmax activation\n",
    "    dAL = AL - Y_one_hot\n",
    "    grads['dAL'] = dAL\n",
    "    \n",
    "    # Last layer (softmax activation)\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, batch_cache, activation_cache = current_cache\n",
    "    dZL_tilde = activation_backward(dAL, activation_cache, \"softmax\")\n",
    "    dZL, dGammaL, dBetaL = batch_normalize_backward(dZL_tilde, batch_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZL, linear_cache, lambd)\n",
    "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}'] = dA_prev, dW, db\n",
    "    grads[f'dGamma{L}'], grads[f'dBeta{L}'] = dGammaL, dBetaL\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        linear_cache, batch_cache, activation_cache = current_cache\n",
    "        dZ_tilde = activation_backward(grads[f'dA{l+1}'], activation_cache, \"relu\")\n",
    "        dZ, dGamma, dBeta = batch_normalize_backward(dZ_tilde, batch_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)\n",
    "        grads[f'dA{l}'], grads[f'dW{l+1}'], grads[f'db{l+1}'] = dA_prev, dW, db\n",
    "        grads[f'dGamma{l+1}'], grads[f'dBeta{l+1}'] = dGamma, dBeta\n",
    "\n",
    "    return grads\n",
    "\n",
    "def initialize_adam(parameters):\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        v[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n",
    "        v[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n",
    "        v[f'dGamma{l}'] = np.zeros_like(parameters[f'gamma{l}'])\n",
    "        v[f'dBeta{l}'] = np.zeros_like(parameters[f'beta{l}'])\n",
    "\n",
    "        s[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n",
    "        s[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n",
    "        s[f'dGamma{l}'] = np.zeros_like(parameters[f'gamma{l}'])\n",
    "        s[f'dBeta{l}'] = np.zeros_like(parameters[f'beta{l}'])\n",
    "\n",
    "    return v, s\n",
    "\n",
    "def update_parameters_adam(parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(parameters) // 4  # Considering gamma and beta for each layer\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        v[f'dW{l}'] = beta1 * v[f'dW{l}'] + (1 - beta1) * grads[f'dW{l}']\n",
    "        v[f'db{l}'] = beta1 * v[f'db{l}'] + (1 - beta1) * grads[f'db{l}']\n",
    "        v[f'dGamma{l}'] = beta1 * v[f'dGamma{l}'] + (1 - beta1) * grads[f'dGamma{l}']\n",
    "        v[f'dBeta{l}'] = beta1 * v[f'dBeta{l}'] + (1 - beta1) * grads[f'dBeta{l}']\n",
    "\n",
    "        s[f'dW{l}'] = beta2 * s[f'dW{l}'] + (1 - beta2) * (grads[f'dW{l}']**2)\n",
    "        s[f'db{l}'] = beta2 * s[f'db{l}'] + (1 - beta2) * (grads[f'db{l}']**2)\n",
    "        s[f'dGamma{l}'] = beta2 * s[f'dGamma{l}'] + (1 - beta2) * (grads[f'dGamma{l}']**2)\n",
    "        s[f'dBeta{l}'] = beta2 * s[f'dBeta{l}'] + (1 - beta2) * (grads[f'dBeta{l}']**2)\n",
    "\n",
    "        v_corrected_dW = v[f'dW{l}'] / (1 - beta1**t)\n",
    "        v_corrected_db = v[f'db{l}'] / (1 - beta1**t)\n",
    "        v_corrected_dGamma = v[f'dGamma{l}'] / (1 - beta1**t)\n",
    "        v_corrected_dBeta = v[f'dBeta{l}'] / (1 - beta1**t)\n",
    "\n",
    "        s_corrected_dW = s[f'dW{l}'] / (1 - beta2**t)\n",
    "        s_corrected_db = s[f'db{l}'] / (1 - beta2**t)\n",
    "        s_corrected_dGamma = s[f'dGamma{l}'] / (1 - beta2**t)\n",
    "        s_corrected_dBeta = s[f'dBeta{l}'] / (1 - beta2**t)\n",
    "\n",
    "        parameters[f'W{l}'] -= learning_rate * v_corrected_dW / (np.sqrt(s_corrected_dW) + epsilon)\n",
    "        parameters[f'b{l}'] -= learning_rate * v_corrected_db / (np.sqrt(s_corrected_db) + epsilon)\n",
    "        parameters[f'gamma{l}'] -= learning_rate * v_corrected_dGamma / (np.sqrt(s_corrected_dGamma) + epsilon)\n",
    "        parameters[f'beta{l}'] -= learning_rate * v_corrected_dBeta / (np.sqrt(s_corrected_dBeta) + epsilon)\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def nn_deep_model_adam_with_regularization(X_train, Y_train, X_test, Y_test, layer_dims, num_iterations=1000, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, lambd=0):\n",
    "    activations = [\"relu\"] * (len(layer_dims) - 2) + [\"softmax\"]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "    # Initialize Adam variables\n",
    "    v, s = initialize_adam(parameters)\n",
    "    t = 0  # Initialize timestep for Adam\n",
    "\n",
    "    # Train the neural network with Adam and L2 regularization\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation_deep(X_train, parameters, activations)\n",
    "        cost = compute_cost(AL, Y_train, parameters, lambd)\n",
    "        grads = backward_propagation_deep(AL, Y_train, caches, activations, lambd)\n",
    "\n",
    "        # Update parameters with Adam\n",
    "        t += 1\n",
    "        parameters = update_parameters_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "            print(f'Mean activation: {np.mean(AL)}, Std activation: {np.std(AL)}')\n",
    "            print(f'Mean dAL: {np.mean(grads[\"dAL\"])}, Std dAL: {np.std(grads[\"dAL\"])}')\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = predict_deep(parameters, X_train, activations)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = predict_deep(parameters, X_test, activations)\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_train), np.squeeze(train_predictions)))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(np.squeeze(Y_test), np.squeeze(test_predictions)))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(np.squeeze(Y_test), np.squeeze(test_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb33f54-97bc-4833-a997-afaa32f95249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 9.704800804492674\n",
      "Mean activation: 0.25, Std activation: 0.14852017421167846\n",
      "Mean dAL: 2.220446049250313e-18, Std dAL: 0.44923822339161956\n",
      "Cost after iteration 1000: 27.848726252952115\n",
      "Mean activation: 0.25, Std activation: 0.38469227056651\n",
      "Mean dAL: -1.3877787807814457e-19, Std dAL: 0.18378117660585613\n",
      "Cost after iteration 2000: 35.768233941604755\n",
      "Mean activation: 0.25, Std activation: 0.39182158528211347\n",
      "Mean dAL: 6.938893903907229e-19, Std dAL: 0.1705053773646197\n",
      "Cost after iteration 3000: 40.05451423784567\n",
      "Mean activation: 0.25, Std activation: 0.3946791239366064\n",
      "Mean dAL: 2.0816681711721684e-19, Std dAL: 0.16919314821963594\n",
      "Cost after iteration 4000: 47.9020088312395\n",
      "Mean activation: 0.25, Std activation: 0.39776961899360763\n",
      "Mean dAL: -5.551115123125783e-19, Std dAL: 0.18809237677299404\n",
      "Cost after iteration 5000: 44.64427657164002\n",
      "Mean activation: 0.25, Std activation: 0.39676374621564453\n",
      "Mean dAL: 2.0816681711721684e-19, Std dAL: 0.16088414218484123\n",
      "Training Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[393   1   1   0]\n",
      " [  3 391  15   0]\n",
      " [  0  15 364  29]\n",
      " [  0   1  43 344]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       395\n",
      "           1       0.96      0.96      0.96       409\n",
      "           2       0.86      0.89      0.88       408\n",
      "           3       0.92      0.89      0.90       388\n",
      "\n",
      "    accuracy                           0.93      1600\n",
      "   macro avg       0.93      0.93      0.93      1600\n",
      "weighted avg       0.93      0.93      0.93      1600\n",
      "\n",
      "\n",
      "Test Set Evaluation:\n",
      "Confusion Matrix:\n",
      "[[91 14  0  0]\n",
      " [23 52 16  0]\n",
      " [ 0 28 54 10]\n",
      " [ 0  3 23 86]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       105\n",
      "           1       0.54      0.57      0.55        91\n",
      "           2       0.58      0.59      0.58        92\n",
      "           3       0.90      0.77      0.83       112\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.70      0.70      0.70       400\n",
      "weighted avg       0.72      0.71      0.71       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "layer_dims = [X_train_nn.shape[0], 10, 8, 6, 4]\n",
    "nn_deep_model_adam_with_regularization(X_train_nn, y_train_nn, X_test_nn, y_test_nn, layer_dims, num_iterations=6000, learning_rate=0.009, lambd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02f21-eb7e-4fd4-a0b8-b7a942a852d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
