{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ba4109-e401-454b-be02-c5b7e031a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Neural_Networks import Logistic_NN, NN_2_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9aebfe-c73b-404c-9901-82edc52d5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398adce7-6fdf-414c-8670-825fc1d295ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0            842     0          2.2         0   1       0           7    0.6   \n",
       "1           1021     1          0.5         1   0       1          53    0.7   \n",
       "2            563     1          0.5         1   2       1          41    0.9   \n",
       "3            615     1          2.5         0   0       0          10    0.8   \n",
       "4           1821     1          1.2         0  13       1          44    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        188        2  ...         20       756  2549     9     7         19   \n",
       "1        136        3  ...        905      1988  2631    17     3          7   \n",
       "2        145        5  ...       1263      1716  2603    11     2          9   \n",
       "3        131        6  ...       1216      1786  2769    16     8         11   \n",
       "4        141        2  ...       1208      1212  1411     8     2         15   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        0             0     1            1  \n",
       "1        1             1     0            2  \n",
       "2        1             1     0            2  \n",
       "3        1             0     0            2  \n",
       "4        1             1     0            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_train = pd.read_csv('mobile_train.csv')\n",
    "mobile_test = pd.read_csv('mobile_test.csv')\n",
    "mobile_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7505cca7-ba4d-4662-859c-7e3c03ed3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping feature 'mobile_wt' with VIF 12.972548425819065\n",
      "Dropping feature 'px_width' with VIF 11.470014131904488\n",
      "Dropping feature 'sc_h' with VIF 11.086593845458365\n",
      "Dropping feature 'battery_power' with VIF 7.543843177190293\n",
      "Dropping feature 'pc' with VIF 6.050059878559392\n",
      "Dropping feature 'three_g' with VIF 5.930418164840767\n"
     ]
    }
   ],
   "source": [
    "mobile_train_vif = mobile_train.drop(['price_range'], axis=1)\n",
    "\n",
    "def calculate_vif(data_frame):\n",
    "    features = data_frame.columns\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = features\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data_frame.values, i) for i in range(data_frame.shape[1])]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "    \n",
    "def drop_high_vif_features(data_frame, threshold=5):\n",
    "    while True:\n",
    "        vif_results = calculate_vif(data_frame)\n",
    "        max_vif_feature = vif_results.loc[vif_results['VIF'].idxmax(), 'Feature']\n",
    "        max_vif_value = vif_results.loc[vif_results['VIF'].idxmax(), 'VIF']\n",
    "        \n",
    "        if max_vif_value > threshold:\n",
    "            print(f\"Dropping feature '{max_vif_feature}' with VIF {max_vif_value}\")\n",
    "            data_frame = data_frame.drop(columns=max_vif_feature)\n",
    "        else:\n",
    "            break\n",
    "    return data_frame\n",
    "mobile_train_vif = drop_high_vif_features(mobile_train_vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07316abc-cc13-4a04-b0e4-11cb50e483cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vif = mobile_train_vif\n",
    "y_vif = mobile_train['price_range']\n",
    "X_train_vif, X_test_vif, y_train_vif, y_test_vif = train_test_split(X_vif, y_vif, test_size=0.2, random_state=42)\n",
    "## now lets standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_vif_scaled = scaler.fit_transform(X_train_vif)\n",
    "X_test_vif_scaled = scaler.transform(X_test_vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca70da7a-9bb5-409f-90b0-bdcc965d1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the data into dimensions (n,m) where m denoted the number of examples for ease of computation\n",
    "X_train_nn, X_test_nn = X_train_vif_scaled.T, X_test_vif_scaled.T\n",
    "y_train_nn,y_test_nn = y_train_vif.to_numpy().reshape((1,1600)), y_test_vif.to_numpy().reshape((1,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad4c0b2-5c9d-4ba3-8a53-f46e3a333458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Logistic_NN(4, 0.01, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc63796-d462-4bfc-a7e2-1cdac7c728c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iterations: 1.3927288487196372\n",
      "cost after 200 iterations: 1.1071201404733622\n",
      "cost after 400 iterations: 0.9818802462425121\n",
      "cost after 600 iterations: 0.9105988122116826\n",
      "cost after 800 iterations: 0.8622213190840751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.03778215,  0.01316373, -0.02107592,  0.02475468],\n",
       "        [ 0.04654256, -0.04050454,  0.0413891 , -0.01604318],\n",
       "        [-0.00195   ,  0.02253812, -0.04105334,  0.01153104],\n",
       "        [-0.04915216,  0.04581861,  0.02736361, -0.0140979 ],\n",
       "        [-0.01800262,  0.01562074, -0.04441215,  0.04191897],\n",
       "        [-0.02983307,  0.0108382 , -0.09916502,  0.10562712],\n",
       "        [-0.04985427,  0.06953424, -0.05319474,  0.01093517],\n",
       "        [ 0.01748953, -0.08883164,  0.0560741 ,  0.01203026],\n",
       "        [-0.3062876 ,  0.01495908,  0.04980938,  0.2567906 ],\n",
       "        [-1.3917064 , -0.48041048,  0.50123622,  1.37353398],\n",
       "        [-0.01277522, -0.0155625 ,  0.00651936,  0.04153273],\n",
       "        [-0.05317597,  0.06113959, -0.04668044,  0.02492847],\n",
       "        [ 0.0263453 ,  0.02799084, -0.0491576 , -0.01046376],\n",
       "        [-0.03136397,  0.01893887, -0.02703963, -0.0115669 ]]),\n",
       " array([[-0.19440471],\n",
       "        [ 0.20194154],\n",
       "        [ 0.19731197],\n",
       "        [-0.2048488 ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train_nn,y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded3399a-559e-4390-a3ca-9acbd54496e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model1.predict(X_test_nn)\n",
    "model1.accuracy(predictions, y_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2b786-ce16-4c85-b924-99666ba99791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f760312-abb4-4094-aa71-12daa3e49e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "trial = NN_2_layer(num_classes=4, num_hidden=4, learning_rate = 0.009, num_iters = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ba7190-e768-4018-80cb-7a0c7e0a761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iterations: 1.9452863185979419\n",
      "cost after 1000 iterations: 1.0245489233062972\n",
      "cost after 2000 iterations: 0.7720318080265752\n",
      "cost after 3000 iterations: 0.646927920893106\n",
      "cost after 4000 iterations: 0.5725058776055323\n",
      "cost after 5000 iterations: 0.5292718885597846\n",
      "cost after 6000 iterations: 0.5031585969257354\n",
      "cost after 7000 iterations: 0.48599274300321726\n",
      "cost after 8000 iterations: 0.473966609566506\n",
      "cost after 9000 iterations: 0.4648335181707388\n"
     ]
    }
   ],
   "source": [
    "trial.fit(X_train_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3e5188-851b-4c23-95e5-76f142c2eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7975\n"
     ]
    }
   ],
   "source": [
    "pred = trial.predict(X_test_nn, y_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7032c-5fc1-48c4-9d49-4ee7b8444f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c894e800-579d-417b-af6d-262ac3ce2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network():\n",
    "\n",
    "    def __init__(\n",
    "        layer_dims, \n",
    "        num_iterations=1000, \n",
    "        learning_rate=0.01, \n",
    "        beta1=0.9, \n",
    "        beta2=0.999, \n",
    "        epsilon=1e-8\n",
    "    ):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.num_iters = num_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.parameters = {}\n",
    "        self.grads = {}\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "    def batch_normalize(self, Z):\n",
    "        \"\"\"\n",
    "        Helper function to compute the normalized value of Z of any layer\n",
    "        \n",
    "        Input: \n",
    "        the Z matrix corresponding to a specific layer \n",
    "        with dimensions (n_l,m). where n_l - number of units in layer l, m - number of training examples in training data\n",
    "\n",
    "        Output:\n",
    "        The normalized Z matrix with the same dimension as Z\n",
    "        Mean and variance are computed using the Zs corresponding to different training examples (sample size is m)\n",
    "        \n",
    "        \"\"\"\n",
    "        mean = np.mean(Z, axis=1, keepdims=True)\n",
    "        var = np.var(Z, axis=1, keepdims=True)\n",
    "        Z_normalized = (Z - mean) / np.sqrt(var + 1e-8)\n",
    "        return Z_normalized, mean, var\n",
    "\n",
    "    def linear_forward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Helper function used to calculate the Z matrix of a layer l\n",
    "        \n",
    "        Inputs:\n",
    "        A - activations from the previous layer. Dimension - (number of units in layer l-1, total number of examples)\n",
    "        W - weights matrix of current layer. Dimension - (number of units in layer l, number of units in layer l-1)\n",
    "        b - bias of current layer. Dimension - (number of units in layer l, 1)\n",
    "\n",
    "        Outputs:\n",
    "        The Z matrix along with all the inputs stored in cache useful in back propogation\n",
    "        \"\"\"\n",
    "        Z = np.dot(W, A) + b\n",
    "        cache = (A, W, b)\n",
    "        return Z, cache\n",
    "\n",
    "    def batch_normalize_forward(self, Z, gamma, beta):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the Z tilde to be fed to the activation function before feeding to the next layer\n",
    "\n",
    "        Inputs:\n",
    "        Z - The Z matrix computed from linear_forward function\n",
    "        gamma - the parameter gamma associated with normalization. Dimension - (number of units in current layer, 1)\n",
    "        beta - the parameter associated with normalization. Dimension - (number of units in current layer, 1)\n",
    "\n",
    "        Outputs:\n",
    "        Z_tilde - the scaled version of Z_normalized obtained from batch_normalize\n",
    "        \"\"\"\n",
    "        Z_normalized, mean, var = self.batch_normalize(Z)\n",
    "        Z_tilde = gamma * Z_normalized + beta # this is element wise multiplication \n",
    "        cache = (Z, Z_normalized, mean, var, gamma, beta)\n",
    "        return Z_tilde, cache\n",
    "\n",
    "    def activation_forward(self, Z, activation):\n",
    "        \"\"\"\n",
    "        Helper function to compute the activation of Z_tilde obtained from previous function.\n",
    "        Applied the activation based on the choice.\n",
    "        \"\"\"\n",
    "        if activation == \"relu\":\n",
    "            A = self.relu(Z)\n",
    "        elif activation ==\"tanh\":\n",
    "            A = self.tanh(Z)\n",
    "        elif activation == \"softmax\":\n",
    "            A = self.softmax(Z)\n",
    "    \n",
    "        cache = Z\n",
    "        return A, cache\n",
    "\n",
    "    def initialize_parameters_deep(self):\n",
    "        \"\"\"\n",
    "        Initializing the weights using xavier's initialization(for ReLU activation) \n",
    "        for all the layers according the layer dimension info given by layer_dims\n",
    "\n",
    "        Dimensions of parameters used in forward propogation:\n",
    "        Weights - (number of units in current layer, number of units in prev layer)\n",
    "        b - (number of units in current layer, 1)\n",
    "\n",
    "        Dimensions of parameters used in batch normalization:\n",
    "        gamma - (number of units in current layer, 1)\n",
    "        beta - (number of units in current layer, 1)\n",
    "        \"\"\"\n",
    "        L = len(self.layer_dims)\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            # following the logic that the dimension of the weights of any layer will be \n",
    "            # (number of units in current layer, number of units in prev layer)\n",
    "            self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2 / self.layer_dims[l-1])\n",
    "\n",
    "            # b, gamma and beta parameters all will will have a dimension that depends on the number of units in the current layer\n",
    "            # given by (number of units in current layer, 1)\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "            self.parameters[f'gamma{l}'] = np.ones((self.layer_dims[l], 1))\n",
    "            self.parameters[f'beta{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward_propagation_deep(self, X, activations):\n",
    "        \"\"\"\n",
    "\n",
    "        Inputs:\n",
    "        Activations - this is a list of size (number_layers - 1) L which basically tells about the activation to use for each layer\n",
    "        for eg. ['relu', 'relu', 'softmax'] means that the first 2 hidden layers will have relu activation and the\n",
    "        output layer will have softmax activation\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(self.parameters) // 4  # There are 4 parameters for each layer W,b,gamma,beta\n",
    "    \n",
    "        for l in range(1, L):\n",
    "            # A_prev gets updated in each iteration as A keeps getting updated in each iteration\n",
    "            A_prev = A\n",
    "            \n",
    "            # the linear_forward function returns Z, (A_prev, W[l], b[l]) stored in cache\n",
    "            Z, linear_cache = self.linear_forward(A_prev, self.parameters[f'W{l}'], self.parameters[f'b{l}'])\n",
    "            \n",
    "            # the batch_normalize_forward returns Z_tilde, (Z, Z_normalized, mean, var, gamma, beta) stored in cache\n",
    "            Z_tilde, batch_cache = self.batch_normalize_forward(Z, self.parameters[f'gamma{l}'], self.parameters[f'beta{l}'])\n",
    "            \n",
    "            # the activation_forward returns A, (Z) stored in cache\n",
    "            A, activation_cache = self.activation_forward(Z_tilde, \"relu\")\n",
    "\n",
    "            # storing all types of cache to one variable\n",
    "            cache = (linear_cache, batch_cache, activation_cache)\n",
    "\n",
    "            # appending that to the caches list. caches will contain the 3 types of cache for all the layers\n",
    "            # the ith item in caches list will have all the cache pertaining to the ith layer in the network\n",
    "            caches.append(cache)\n",
    "    \n",
    "        # Last layer (softmax activation)\n",
    "        ZL, linear_cache = self.linear_forward(A, self.parameters[f'W{L}'], self.parameters[f'b{L}'])\n",
    "        ZL_tilde, batch_cache = self.batch_normalize_forward(ZL, self.parameters[f'gamma{L}'], self.parameters[f'beta{L}'])\n",
    "        AL, activation_cache = self.activation_forward(ZL_tilde, \"softmax\")\n",
    "        cache = (linear_cache, batch_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Helper function to compute the cost function that compares the activations of the final layer and the target labels.\n",
    "        Assumes that the target labels Y has a dimension (m,) where m denotes the number of examples\n",
    "        \"\"\"\n",
    "        Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "        cost = -1/self.m * np.sum(Y_one_hot * np.log(AL+self.epsilon))\n",
    "        return cost\n",
    "\n",
    "    def activation_backward(self, dA, cache, activation):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the gradients of Z_tilde because we get activations of any layer from Z_tilde\n",
    "        So in backward propogation we get dZ_tilde from dA\n",
    "\n",
    "        Inputs:\n",
    "        dA - derivative of the cost function with respect to the activation of that layer \n",
    "        so for the last layer L it'll be dcost/dAL\n",
    "\n",
    "        Output:\n",
    "        dZ - which is actually dZ_tilde. we get the derivative of the cost function with respect to \n",
    "        Z_tilde of that layer. so for the last layer L it'll be dcost/dZL_tilde = (dcost/dAL)*(dAL/dZL_tilde) by chain rule\n",
    "        \"\"\"\n",
    "        Z = cache\n",
    "        if activation == \"relu\":\n",
    "            dZ = dA * self.relu_derivative(Z)\n",
    "        elif activation == \"tanh\":\n",
    "            dZ = dA * self.tanh_derivative(Z)\n",
    "        elif activation == \"softmax\":\n",
    "            dZ = dA\n",
    "        return dZ\n",
    "\n",
    "    def batch_normalize_backward(self, dZ_tilde, cache):\n",
    "        \"\"\"\n",
    "        Helper function to compute the gradients of Z, gamma, beta because in forward propogation we get Z_tilde from Z, gamma, beta\n",
    "        so in backward propogation we get dZ, dGamma, dBeta from dZ_tilde\n",
    "\n",
    "        Inputs:\n",
    "        dZ_tilde - derivative of the cost function with respect to Z_tilde of that layer. \n",
    "        so for the last layer it will be dcost/dZL_tilde\n",
    "\n",
    "        Output:\n",
    "        dZ - derivative of the cost function with respect to Z of that layer.\n",
    "        so for last layer it will be dcost/dZL = (dcost/dZL_tilde)*(dZL_tilde/dZL)\n",
    "        \"\"\"\n",
    "        Z, Z_normalized, mean, var, gamma, beta = cache\n",
    "    \n",
    "        dZ_normalized = dZ_tilde * gamma # dZL_tilde/dZL = gamma \n",
    "        dVar = np.sum(dZ_normalized * (Z - mean), axis=1, keepdims=True) * -0.5 * (var + 1e-8)**(-1.5)\n",
    "        dMean = np.sum(dZ_normalized, axis=1, keepdims=True) * -1 / np.sqrt(var + 1e-8)\n",
    "        \n",
    "        dZ = (dZ_normalized / np.sqrt(var + 1e-8)) + (dVar * 2 * (Z - mean) / self.m) + (dMean / self.m)\n",
    "        dGamma = np.sum(dZ_tilde * Z_normalized, axis=1, keepdims=True)\n",
    "        dBeta = np.sum(dZ_tilde, axis=1, keepdims=True)\n",
    "    \n",
    "        return dZ, dGamma, dBeta\n",
    "\n",
    "    def linear_backward(self, dZ, cache):\n",
    "        \"\"\"\n",
    "        Helper function to compute the gradients of A_prev, W, b because in forward propogation we get Z from A_prev, W, b\n",
    "        so in backward propogation we get dA_prev, dW, db from dZ\n",
    "\n",
    "        Inputs:\n",
    "        dZ - derivative of the cost function with respect to Z of that layer. \n",
    "        so for the last layer it will be dcost/dZL\n",
    "\n",
    "        Output:\n",
    "        dA_prev, dW, db - derivative of the cost function with respect to W, b of that layer and A of previous layer.\n",
    "        so for last layer it will be dcost/dWL = (dcost/dZL)*(dZL/dWL) and so on for all 3 of them\n",
    "        \"\"\"\n",
    "        A_prev, W, b = cache\n",
    "        dW = 1/self.m * np.dot(dZ, A_prev.T)\n",
    "        db = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def backward_propagation_deep(self, AL, Y, caches, activations):\n",
    "        L = len(caches)\n",
    "    \n",
    "        # Convert Y to one-hot encoded matrix\n",
    "        Y_one_hot = np.eye(AL.shape[0])[Y.astype(int)].T.squeeze()\n",
    "    \n",
    "        # Compute gradient of the cost with respect to AL for softmax activation\n",
    "        dAL = AL - Y_one_hot\n",
    "        self.grads['dAL'] = dAL\n",
    "        \n",
    "        # Last layer (softmax activation)\n",
    "        current_cache = caches[L-1]\n",
    "        linear_cache, batch_cache, activation_cache = current_cache\n",
    "        dZL_tilde = self.activation_backward(dAL, activation_cache, \"softmax\")\n",
    "        dZL, dGammaL, dBetaL = self.batch_normalize_backward(dZL_tilde, batch_cache)\n",
    "        dA_prev, dW, db = self.linear_backward(dZL, linear_cache)\n",
    "        self.grads[f'dA{L-1}'], self.grads[f'dW{L}'], self.grads[f'db{L}'] = dA_prev, dW, db\n",
    "        self.grads[f'dGamma{L}'], self.grads[f'dBeta{L}'] = dGammaL, dBetaL\n",
    "    \n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            linear_cache, batch_cache, activation_cache = current_cache\n",
    "            dZ_tilde = self.activation_backward(grads[f'dA{l+1}'], activation_cache, \"relu\")\n",
    "            dZ, dGamma, dBeta = self.batch_normalize_backward(dZ_tilde, batch_cache)\n",
    "            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "            self.grads[f'dA{l}'], self.grads[f'dW{l+1}'], self.grads[f'db{l+1}'] = dA_prev, dW, db\n",
    "            self.grads[f'dGamma{l+1}'], self.grads[f'dBeta{l+1}'] = dGamma, dBeta\n",
    "    \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcac33-85dc-4952-9d3c-b6ba977163b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7704d-fc7d-4410-b4ab-8f43d85c20b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657e07c-58f4-4d0c-bb23-3bcfeb556f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac826cc-c334-4f5b-b412-eb38ac4418df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f4051-0866-4356-9ede-1d7ff3be7bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc29581-72cf-4a25-b81d-c3fdca28f566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
